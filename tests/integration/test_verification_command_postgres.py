"""Integration tests for verification CLI command against PostgreSQL.

These tests validate that the verification command works correctly with
PostgreSQL/Cloud SQL, including:
- Status summary queries
- URL verification workflow
- Telemetry recording
- PostgreSQL-specific features (e.g., FOR UPDATE SKIP LOCKED)

Following the test development protocol from .github/copilot-instructions.md:
1. Uses cloud_sql_session fixture for PostgreSQL with automatic rollback
2. Creates all required parent records (Source, CandidateLink)
3. Marks with @pytest.mark.postgres AND @pytest.mark.integration
4. Tests run in postgres-integration CI job with PostgreSQL 15
"""

import uuid
from datetime import datetime, timezone
from unittest.mock import Mock, patch

import pytest
from sqlalchemy import text

from src.cli.commands.verification import (
    handle_verification_command,
    show_verification_status,
)
from src.models import CandidateLink, Source, VerificationPattern

# Mark all tests to require PostgreSQL and run in integration job
pytestmark = [pytest.mark.postgres, pytest.mark.integration]


@pytest.fixture
def test_source(cloud_sql_session):
    """Create a test source for verification tests."""
    source = Source(
        id=str(uuid.uuid4()),
        host="test-verification.example.com",
        host_norm="test-verification.example.com",
        canonical_name="Test Verification Source",
        city="Test City",
        county="Test County",
    )
    cloud_sql_session.add(source)
    cloud_sql_session.commit()
    cloud_sql_session.refresh(source)
    return source


@pytest.fixture
def discovered_candidates(cloud_sql_session, test_source):
    """Create discovered candidate links for verification testing."""
    candidates = []
    for i in range(5):
        candidate = CandidateLink(
            id=str(uuid.uuid4()),
            url=f"https://test-verification.example.com/article-{i}",
            source=test_source.canonical_name,
            source_host_id=test_source.id,
            crawl_depth=0,
            status="discovered",
            discovered_at=datetime.now(timezone.utc),
            discovered_by="test_verification",
        )
        candidates.append(candidate)
        cloud_sql_session.add(candidate)

    cloud_sql_session.commit()
    for candidate in candidates:
        cloud_sql_session.refresh(candidate)
    return candidates


class TestVerificationStatusPostgres:
    """Test verification status command with PostgreSQL."""

    def test_show_verification_status_queries_postgres(
        self, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test that status queries work against PostgreSQL."""
        # Create a mock service that uses the real database
        from src.services.url_verification import URLVerificationService

        # We need to inject the session into the service, but the service
        # creates its own DatabaseManager. For this test, we'll test the
        # query directly.
        # Test the status summary query against PostgreSQL
        query = text(
            """
            SELECT status, COUNT(*) as count
            FROM candidate_links
            WHERE source_host_id = :source_id
            GROUP BY status
            ORDER BY count DESC
        """
        )

        result = cloud_sql_session.execute(query, {"source_id": test_source.id})
        status_counts = {row[0]: row[1] for row in result}

        # Verify the discovered status
        assert status_counts.get("discovered", 0) == 5
        assert sum(status_counts.values()) == 5

    def test_verification_pending_count_postgres(
        self, cloud_sql_session, discovered_candidates
    ):
        """Test counting pending verification URLs in PostgreSQL."""
        # Query for pending verification (discovered status)
        query = text(
            """
            SELECT COUNT(*)
            FROM candidate_links
            WHERE status = 'discovered'
        """
        )

        result = cloud_sql_session.execute(query)
        count = result.scalar()

        assert count == 5

    def test_verification_status_breakdown_postgres(
        self, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test status breakdown query with PostgreSQL aggregation."""
        # Add some verified candidates
        verified_candidate = CandidateLink(
            id=str(uuid.uuid4()),
            url="https://test-verification.example.com/verified-article",
            source=test_source.canonical_name,
            source_host_id=test_source.id,
            crawl_depth=0,
            status="article",
            discovered_at=datetime.now(timezone.utc),
            discovered_by="test_verification",
            processed_at=datetime.now(timezone.utc),
        )
        cloud_sql_session.add(verified_candidate)
        cloud_sql_session.commit()

        # Query status breakdown
        query = text(
            """
            SELECT status, COUNT(*) as count
            FROM candidate_links
            WHERE source_host_id = :source_id
            GROUP BY status
            ORDER BY status
        """
        )

        result = cloud_sql_session.execute(query, {"source_id": test_source.id})
        status_breakdown = list(result)

        # Should have both 'discovered' and 'article' statuses
        statuses = {row[0]: row[1] for row in status_breakdown}
        assert statuses.get("discovered", 0) == 5
        assert statuses.get("article", 0) == 1


class TestVerificationCommandPostgres:
    """Test verification command execution with PostgreSQL."""

    @patch("src.cli.commands.verification.URLVerificationService")
    def test_verification_command_with_status_flag(
        self, mock_service_class, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test verification command --status flag with PostgreSQL data."""
        # Mock the service to return real data
        mock_service = Mock()
        mock_service_class.return_value = mock_service

        # Return status summary matching our test data
        mock_service.get_status_summary.return_value = {
            "total_urls": 5,
            "verification_pending": 5,
            "articles_verified": 0,
            "non_articles_verified": 0,
            "verification_failures": 0,
            "status_breakdown": {
                "discovered": 5,
            },
        }

        # Create mock args
        args = Mock()
        args.batch_size = 100
        args.sleep_interval = 30
        args.max_batches = None
        args.log_level = "INFO"
        args.status = True
        args.continuous = False
        args.idle_grace_seconds = 0

        # Patch logging to avoid side effects
        with patch("src.cli.commands.verification.logging.basicConfig"):
            exit_code = handle_verification_command(args)

        # Should exit successfully
        assert exit_code == 0
        # Service should have been called with correct parameters
        mock_service_class.assert_called_once_with(batch_size=100, sleep_interval=30)
        mock_service.get_status_summary.assert_called_once()


class TestVerificationPostgresFeatures:
    """Test PostgreSQL-specific verification features."""

    def test_dynamic_pattern_filters_obituary_urls(
        self,
        cloud_sql_session,
        test_source,
        discovered_candidates,
        monkeypatch,
    ):
        """Ensure DB-backed verification patterns short-circuit StorySniffer."""

        pattern = VerificationPattern(
            pattern_type="obituary",
            pattern_regex="/obits/",
            pattern_description="Matches obituary URLs",
            is_active=True,
        )
        cloud_sql_session.add(pattern)
        cloud_sql_session.flush()

        target_url = "https://test-verification.example.com/news/obits/john-doe"

        class _PatchedDatabaseManager:
            def __init__(self):
                self.engine = cloud_sql_session.get_bind()

            def get_session(self):
                from contextlib import contextmanager

                @contextmanager
                def _session_context():
                    yield cloud_sql_session

                return _session_context()

        from src.services.url_verification import URLVerificationService

        monkeypatch.setattr(
            "src.services.url_verification.DatabaseManager",
            lambda *_, **__: _PatchedDatabaseManager(),
        )

        class _FakeTelemetry:
            def record_verification_batch(self, *args, **kwargs):
                return None

        service = URLVerificationService(
            run_http_precheck=False, telemetry_tracker=_FakeTelemetry()
        )
        service._pattern_cache = None
        service._pattern_cache_expiry = 0

        monkeypatch.setattr(
            service.sniffer,
            "guess",
            lambda _: (_ for _ in ()).throw(
                AssertionError("StorySniffer should not execute for pattern hits")
            ),
        )

        first_result = service.verify_url(target_url)
        assert first_result["pattern_filtered"] is True
        assert first_result["pattern_status"] == "obituary"
        assert first_result["pattern_type"] == "obituary"
        assert first_result["pattern_id"] == pattern.id
        assert first_result["storysniffer_result"] is False
        assert first_result["error"] is None

        second_result = service.verify_url(target_url)
        assert second_result["pattern_filtered"] is True
        assert second_result["pattern_status"] == "obituary"
        assert second_result["pattern_id"] == pattern.id
        assert second_result["error"] is None

    def test_candidate_link_ordering_postgres(
        self, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test that candidates are ordered correctly for verification."""
        # Query candidates in the order they would be processed
        query = text(
            """
            SELECT id, url, discovered_at
            FROM candidate_links
            WHERE status = 'discovered'
            AND source_host_id = :source_id
            ORDER BY discovered_at ASC, id ASC
            LIMIT 10
        """
        )

        result = cloud_sql_session.execute(query, {"source_id": test_source.id})
        candidates = list(result)

        # Should return all 5 candidates
        assert len(candidates) == 5

        # Verify ordering by checking timestamps are monotonic
        timestamps = [row[2] for row in candidates]
        assert timestamps == sorted(timestamps)

    def test_for_update_skip_locked_syntax_postgres(
        self, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test PostgreSQL FOR UPDATE SKIP LOCKED for parallel processing.

        This syntax is PostgreSQL-specific and allows multiple workers to
        process different batches without blocking each other.
        """
        # Test the FOR UPDATE SKIP LOCKED query that would be used
        # in production for parallel verification workers
        query = text(
            """
            SELECT id, url, source_host_id, status
            FROM candidate_links
            WHERE status = 'discovered'
            AND source_host_id = :source_id
            ORDER BY discovered_at ASC
            LIMIT 2
            FOR UPDATE SKIP LOCKED
        """
        )

        # This should succeed without error in PostgreSQL
        result = cloud_sql_session.execute(query, {"source_id": test_source.id})
        locked_candidates = list(result)

        # Should lock 2 candidates for processing
        assert len(locked_candidates) == 2

        # All should be in discovered status
        for row in locked_candidates:
            assert row[3] == "discovered"

    def test_verification_update_with_timestamp_postgres(
        self, cloud_sql_session, discovered_candidates
    ):
        """Test updating verification status with PostgreSQL timestamp."""
        candidate = discovered_candidates[0]

        # Update using PostgreSQL CURRENT_TIMESTAMP
        update_query = text(
            """
            UPDATE candidate_links
            SET status = 'article',
                processed_at = CURRENT_TIMESTAMP
            WHERE id = :candidate_id
            RETURNING processed_at
        """
        )

        result = cloud_sql_session.execute(update_query, {"candidate_id": candidate.id})
        cloud_sql_session.commit()

        # Get the returned timestamp
        processed_at = result.fetchone()[0]
        assert processed_at is not None

        # Verify update in database
        cloud_sql_session.refresh(candidate)
        assert candidate.status == "article"
        assert candidate.processed_at is not None


class TestVerificationTelemetryPostgres:
    """Test verification telemetry with PostgreSQL."""

    def test_telemetry_aggregation_by_source_postgres(
        self, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test aggregating verification telemetry by source in PostgreSQL."""
        # Create a second source with candidates
        source2 = Source(
            id=str(uuid.uuid4()),
            host="test-verification2.example.com",
            host_norm="test-verification2.example.com",
            canonical_name="Test Verification Source 2",
            city="Test City 2",
            county="Test County 2",
        )
        cloud_sql_session.add(source2)

        # Add candidates to second source
        for i in range(3):
            candidate = CandidateLink(
                id=str(uuid.uuid4()),
                url=f"https://test-verification2.example.com/article-{i}",
                source=source2.canonical_name,
                source_host_id=source2.id,
                crawl_depth=0,
                status="discovered",
                discovered_at=datetime.now(timezone.utc),
                discovered_by="test_verification",
            )
            cloud_sql_session.add(candidate)

        cloud_sql_session.commit()

        # Query telemetry aggregation
        query = text(
            """
            SELECT source_host_id, COUNT(*) as candidate_count
            FROM candidate_links
            WHERE status = 'discovered'
            GROUP BY source_host_id
            ORDER BY candidate_count DESC
        """
        )

        result = cloud_sql_session.execute(query)
        telemetry = list(result)

        # Should have 2 sources
        assert len(telemetry) == 2

        # Verify counts
        counts = {row[0]: row[1] for row in telemetry}
        assert counts[test_source.id] == 5
        assert counts[source2.id] == 3

    def test_verification_time_tracking_postgres(
        self, cloud_sql_session, test_source, discovered_candidates
    ):
        """Test tracking verification timing in PostgreSQL."""
        # This tests that PostgreSQL interval and timestamp operations work
        candidate = discovered_candidates[0]

        # Update with verification timing
        update_query = text(
            """
            UPDATE candidate_links
            SET status = 'article',
                processed_at = CURRENT_TIMESTAMP,
                error_message = NULL
            WHERE id = :candidate_id
            RETURNING processed_at
        """
        )

        result = cloud_sql_session.execute(update_query, {"candidate_id": candidate.id})
        result.fetchone()  # Execute the query but don't store result
        cloud_sql_session.commit()

        # Query to find recently processed candidates (using INTERVAL)
        recent_query = text(
            """
            SELECT id, url, processed_at
            FROM candidate_links
            WHERE processed_at >= CURRENT_TIMESTAMP - INTERVAL '5 minutes'
            AND source_host_id = :source_id
        """
        )

        result = cloud_sql_session.execute(recent_query, {"source_id": test_source.id})
        recent_verifications = list(result)

        # Should find the recently verified candidate
        assert len(recent_verifications) >= 1
        assert candidate.id in [row[0] for row in recent_verifications]

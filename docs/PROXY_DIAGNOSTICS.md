# Proxy and Anti-Bot Detection Diagnostics

This document explains how to diagnose proxy and anti-bot detection issues using the enhanced logging and diagnostic tools added to address Issue #62.

## Overview

The system uses several anti-bot detection measures:
1. **Squid Proxy** (configured via `SQUID_PROXY_URL`) - Routes requests through the residential proxy tier
2. **Cloudscraper** - Bypasses Cloudflare protection
3. **User-Agent Rotation** - Rotates browser User-Agents per domain
4. **Rate Limiting & Backoff** - Exponential backoff for failed requests
5. **Session Management** - Domain-specific sessions with clean cookies

## Enhanced Logging

The system now includes comprehensive logging with emoji indicators for easy scanning:

### Session Creation
```
ğŸ”§ Created new cloudscraper session (anti-Cloudflare enabled)
ğŸ”€ Squid proxy routing enabled (proxy: http://squid.proxy.net:3128)
```

### Domain Sessions
```
ğŸ”§ Created cloudscraper session for example.com (proxy: squid, UA: Mozilla/5.0...)
```

### Proxy Usage
```
ğŸ”€ Proxying GET example.com via http://squid.proxy.net:3128 (auth: yes)
âœ“ Proxy response 200 for example.com
âœ— Proxy request failed for example.com: ConnectionError: ...
```

### HTTP Requests
```
ğŸ“¡ Fetching https://example.com/article... via session for example.com
ğŸ“¥ Received 200 for example.com (content: 45678 bytes)
âœ… Successfully fetched 45678 bytes from example.com (UA: Mozilla/5.0...)
```

### Bot Detection
```
ğŸš« Bot detection (403) by example.com - response preview: <html><body>Access Denied...
CAPTCHA backoff for example.com: 900s (attempt 1)
```

## Diagnostic Script

Run the diagnostic script to test proxy configuration and connectivity:

```bash
# Set up environment (if not already configured)
export PROXY_PROVIDER=squid
export SQUID_PROXY_URL=http://squid.proxy.net:3128
export SQUID_PROXY_USERNAME=your_username   # optional
export SQUID_PROXY_PASSWORD=your_password   # optional

# Run diagnostics
python scripts/diagnose_proxy.py
```

The script will:
1. âœ“ Check environment variables
2. âœ“ Test proxy server connectivity
3. âœ“ Test cloudscraper availability
4. âœ“ Test proxied requests to test sites
5. âœ“ Test fetching real news sites
6. âœ“ Provide recommendations

### Expected Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    PROXY DIAGNOSTIC TOOL                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

================================================================================
ENVIRONMENT VARIABLES
================================================================================
   âœ“ PROXY_PROVIDER=squid
   âœ“ SQUID_PROXY_URL=http://squid.proxy.net:3128
   âœ“ SQUID_PROXY_USERNAME=your_username
   âœ“ SQUID_PROXY_PASSWORD=***********

================================================================================
PROXY CONNECTIVITY TEST
================================================================================
Testing connectivity to: http://squid.proxy.net:3128
  âœ“ Test request succeeded (status: 200)

================================================================================
PROXIED REQUEST TEST
================================================================================
Active provider: squid

Testing: http://httpbin.org/ip
  âœ“ Status: 200
  Content length: 35 bytes

================================================================================
REAL SITE TEST
================================================================================
Testing: https://www.example.com/
  Proxy enabled: True
  âœ“ Status: 200
  Content length: 1256 bytes
```

## Monitoring Processor Logs

When the processor is running, monitor logs to verify the system is working:

### 1. Check Proxy is Being Used

Look for proxy logging messages:
```bash
kubectl logs -n production -l app=mizzou-processor --tail=100 | grep "Proxying"
```

Expected output:
```
ğŸ”€ Proxying GET fultonsun.com via http://proxy.kiesow.net:23432 (auth: yes)
ğŸ”€ Proxying GET newstribune.com via http://proxy.kiesow.net:23432 (auth: yes)
```

**âš ï¸ Warning Signs:**
- `auth: NO - MISSING CREDENTIALS` - Credentials not configured
- No "Proxying" messages - Proxy not being used at all
- Messages show `bypassed` - URLs are being bypassed incorrectly

### 2. Check Authentication

Verify credentials are present:
```bash
kubectl get secret -n production squid-proxy-credentials -o jsonpath='{.data}' | jq
```

### 3. Check Cloudscraper

Look for cloudscraper usage:
```bash
kubectl logs -n production -l app=mizzou-processor --tail=100 | grep "cloudscraper"
```

Expected output:
```
ğŸ”§ Created new cloudscraper session (anti-Cloudflare enabled)
ğŸ”§ Created cloudscraper session for fultonsun.com (proxy: enabled, ...)
```

**âš ï¸ Warning Signs:**
- `cloudscraper NOT available` - Package not installed
- `Created new requests session` - Falling back to basic requests

### 4. Check Rate Limiting and Backoff

Look for backoff messages:
```bash
kubectl logs -n production -l app=mizzou-processor --tail=100 | grep "backoff\|rate limit"
```

Expected output:
```
ğŸš« Bot detection (403) by fultonsun.com - response preview: ...
CAPTCHA backoff for fultonsun.com: 900s (attempt 1)
Rate limited by ozarksfirst.com, backing off for 60s (attempt 1)
```

### 5. Check Success Rate

Count successful vs failed fetches:
```bash
# Successful fetches
kubectl logs -n production -l app=mizzou-processor --tail=500 | grep -c "Successfully fetched"

# Failed fetches
kubectl logs -n production -l app=mizzou-processor --tail=500 | grep -c "Bot detection"
```

## Common Issues and Solutions

### Issue 1: Proxy Not Being Used

**Symptoms:**
- No "Proxying" messages in logs
- Direct connection errors

**Solutions:**
1. Ensure `PROXY_PROVIDER` is set to `squid`
2. Verify `SQUID_PROXY_URL` (and credentials, if required) are present
3. Check deployment YAML mounts the `squid-proxy-credentials` secret via env vars

### Issue 2: 400 BAD REQUEST from Proxy

**Symptoms:**
```
âœ— Proxy request failed for example.com: 400 Client Error: BAD REQUEST
```

**Possible Causes:**
1. Malformed URL encoding
2. Proxy authentication issue
3. Proxy server configuration issue
4. URL contains characters proxy can't handle

**Solutions:**
1. Check proxy logs for more details
2. Verify authentication credentials are correct
3. Test with diagnostic script
4. Check if specific URLs are causing issues

### Issue 3: Missing Authentication

**Symptoms:**
```
ğŸ”€ Proxying GET example.com via http://proxy.kiesow.net:23432 (auth: NO - MISSING CREDENTIALS)
```

**Solutions:**
1. Set `PROXY_USERNAME` and `PROXY_PASSWORD` environment variables
2. Check Kubernetes secret `squid-proxy-credentials` exists
3. Verify secret is mounted in deployment

### Issue 4: Cloudflare CAPTCHA

**Symptoms:**
```
ğŸš« Bot detection (403) by fultonsun.com - response preview: <html>...Cloudflare...
CAPTCHA backoff for fultonsun.com: 900s (attempt 1)
```

**Solutions:**
1. Verify cloudscraper is installed and being used
2. Increase backoff timers: `CAPTCHA_BACKOFF_BASE`, `CAPTCHA_BACKOFF_MAX`
3. Consider residential proxy service
4. Check if User-Agent rotation is working
5. May need CAPTCHA solving service for persistent blocks

### Issue 5: Rate Limiting (403)

**Symptoms:**
```
ğŸš« Bot detection (403) by ozarksfirst.com
Rate limited, backing off for 60s (attempt 1)
```

**Solutions:**
1. Backoff will automatically increase exponentially
2. Verify inter-request delays: `INTER_REQUEST_MIN`, `INTER_REQUEST_MAX`
3. Check if domain has very strict rate limits
4. May need to reduce discovery frequency for this source

## Configuration Reference

Environment variables that control proxy and anti-bot behavior:

### Proxy Configuration
- `PROXY_PROVIDER` - Active provider (must be `squid` in production)
- `SQUID_PROXY_URL` - Proxy server URL (e.g., http://squid.proxy.net:3128)
- `SQUID_PROXY_USERNAME` / `SQUID_PROXY_PASSWORD` - Optional credentials
- `PROXY_USERNAME` - Proxy authentication username
- `PROXY_PASSWORD` - Proxy authentication password
- `NO_PROXY` - Comma-separated list of hosts to bypass

### Rate Limiting
- `INTER_REQUEST_MIN` - Minimum delay between requests (default: 1.5s)
- `INTER_REQUEST_MAX` - Maximum delay between requests (default: 3.5s)
- `CAPTCHA_BACKOFF_BASE` - Base backoff for CAPTCHA (default: 600s / 10min)
- `CAPTCHA_BACKOFF_MAX` - Max backoff for CAPTCHA (default: 5400s / 90min)

### User-Agent Rotation
- `UA_ROTATE_BASE` - Requests before rotating UA (default: 9)
- `UA_ROTATE_JITTER` - Jitter factor for rotation (default: 0.25)

### Session Management
- `REQUEST_TIMEOUT` - HTTP request timeout (default: 20s)
- `DEAD_URL_TTL_SECONDS` - Cache dead URLs for this long (default: 604800s / 7 days)

## Testing Changes

After making configuration changes:

1. **Restart processor:**
   ```bash
   kubectl rollout restart deployment/mizzou-processor -n production
   ```

2. **Watch logs:**
   ```bash
   kubectl logs -n production -l app=mizzou-processor -f
   ```

3. **Look for new log messages:**
   - Check proxy is being used
   - Verify authentication is present
   - Monitor success rate
   - Check for bot detection

4. **Monitor metrics:**
   - Check extraction success rate in telemetry
   - Monitor backoff timers
   - Track which domains are failing

## Recommendations

Based on the logging and diagnostics:

1. **Verify proxy is working:**
   - Run diagnostic script
   - Check for "Proxying" messages in logs
   - Verify authentication is present

2. **Monitor backoff behavior:**
   - Check backoff timers are increasing
   - Verify domains respect backoff periods
   - Adjust timers if needed

3. **Track domain success rates:**
   - Identify problematic domains
   - Consider pausing consistently failing sources
   - Focus on high-success domains

4. **Consider upgrades:**
   - Residential proxy service (expensive but effective)
   - CAPTCHA solving service (for persistent blocks)
   - More aggressive User-Agent rotation

5. **Implement retry limits:**
   - Add `retry_count` column to track failures
   - Mark as `extraction_failed` after N attempts
   - Prevents queue from clogging with unfetchable articles

## Related Files

- `src/crawler/proxy_config.py` - Proxy manager implementation
- `src/crawler/__init__.py` - ContentExtractor with Squid routing
- `scripts/diagnose_proxy.py` - Diagnostic tool
- `docs/PROXY_CONFIGURATION_CRITICAL.md` - Production guardrails
- `k8s/processor-deployment.yaml` - Deployment configuration

## Related Issues

- Issue #62: Extraction degraded due to anti-bot protection
- Issue #57: Processor errors (resolved)
- Issue #56: Pipeline visibility improvements

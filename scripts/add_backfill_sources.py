#!/usr/bin/env python3
"""
Add new sources from MO_backfill.xlsx to the database.
Avoids duplicates by checking existing hosts.
"""

import pandas as pd
from urllib.parse import urlparse
import subprocess
import json

# Read backfill file
backfill_path = '/Users/kiesowd/Library/Mobile Documents/com~apple~CloudDocs/Documents/Mizzou/LNIC/MO_backfill.xlsx'
df = pd.read_excel(backfill_path)

# Extract hosts
df['host'] = df['url_news'].apply(lambda x: urlparse(str(x)).netloc.replace('www.', ''))

print(f"Backfill file: {len(df)} sources")

# Get existing sources from production
print("Fetching existing sources from production...")
result = subprocess.run(
    ['kubectl', 'exec', '-n', 'production', 'deployment/mizzou-api', '--', 
     'python', '-m', 'src.cli.cli_modular', 'list-sources', '--format', 'json'],
    capture_output=True, 
    text=True
)

# Parse JSON (skip log lines, find JSON array)
lines = result.stdout.strip().split('\n')
json_start = next(i for i, line in enumerate(lines) if line.startswith('['))
json_end = next((i for i in range(len(lines) - 1, json_start, -1) if lines[i].strip() == ']'), len(lines))
json_text = '\n'.join(lines[json_start:json_end + 1])
existing_sources = json.loads(json_text)

existing_hosts = {s['host'] for s in existing_sources}
print(f"Existing sources in database: {len(existing_hosts)}")

# Find new sources
new_sources = df[~df['host'].isin(existing_hosts)].copy()
existing_in_backfill = df[df['host'].isin(existing_hosts)]

print(f"\nNEW sources to add: {len(new_sources)}")
print(f"EXISTING sources (skip): {len(existing_in_backfill)}")

if len(existing_in_backfill) > 0:
    print("\nExisting sources (will skip):")
    for _, row in existing_in_backfill.iterrows():
        print(f"  âœ“ {row['name']:40s} ({row['host']})")

if len(new_sources) > 0:
    print("\nNew sources to add:")
    for _, row in new_sources.iterrows():
        print(f"  + {row['name']:40s} ({row['host']})")
    
    # Save to CSV for load-sources command (match expected format)
    output_file = '/Users/kiesowd/VSCode/NewsCrawler/MizzouNewsCrawler-Scripts/sources/mo_backfill_new_sources.csv'
    new_sources_for_import = new_sources[['name', 'city', 'county', 'url_news', 'host']].copy()
    new_sources_for_import.columns = ['name', 'city', 'county', 'url_news', 'host']
    # Add host_id (UUID will be generated by database)
    import uuid
    new_sources_for_import['host_id'] = [str(uuid.uuid4()) for _ in range(len(new_sources_for_import))]
    # Reorder columns to match expected format
    new_sources_for_import = new_sources_for_import[['host_id', 'name', 'city', 'county', 'url_news']]
    new_sources_for_import.to_csv(output_file, index=False)
    
    print(f"\nSaved {len(new_sources)} new sources to:")
    print(f"  {output_file}")
    print("\nTo add these sources to the database, run:")
    print(f"  kubectl exec -n production deployment/mizzou-api -- python -m src.cli.cli_modular load-sources --file sources/mo_backfill_new_sources.csv")
else:
    print("\nAll backfill sources already exist in the database!")

apiVersion: batch/v1
kind: Job
metadata:
  name: mizzou-discovery
  namespace: production
  labels:
    app: mizzou-discovery
    dataset: mizzou-missouri-state
    type: discovery
spec:
  ttlSecondsAfterFinished: 86400  # Clean up after 24 hours
  template:
    metadata:
      labels:
        app: mizzou-discovery
        dataset: mizzou-missouri-state
        type: discovery
    spec:
      serviceAccountName: mizzou-app
      priorityClassName: batch-standard  # Can be preempted by services/cron jobs
      restartPolicy: Never
      containers:
      - name: discovery
        image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
        imagePullPolicy: Always
        command:
          - python
          - -m
          - src.cli.cli_modular
          - discover-urls
          - --dataset
          - "Mizzou Missouri State"
          - --max-articles
          - "50"  # Max articles per source
          - --days-back
          - "7"   # Look back 7 days
        env:
        # Database configuration
        - name: DATABASE_ENGINE
          value: "postgresql+psycopg2"
        - name: DATABASE_HOST
          value: "127.0.0.1"
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: username
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: password
        - name: DATABASE_NAME
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: database
        # Cloud SQL Connector
        - name: USE_CLOUD_SQL_CONNECTOR
          value: "true"
        - name: CLOUD_SQL_INSTANCE
          value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
        # Proxy configuration
        - name: PROXY_PROVIDER
          value: "squid"
        - name: SQUID_PROXY_URL
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: squid-proxy-url
              optional: true
        - name: SQUID_PROXY_USERNAME
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: username
              optional: true
        - name: SQUID_PROXY_PASSWORD
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: password
              optional: true
        - name: SELENIUM_PROXY
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: selenium-proxy-url
              optional: true
        # Rate limiting (moderate for discovery)
        - name: INTER_REQUEST_MIN
          value: "2.0"
        - name: INTER_REQUEST_MAX
          value: "5.0"
        # Bypass proxy for internal services
        - name: NO_PROXY
          value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
        resources:
          requests:
            cpu: 100m
            memory: 768Mi
          limits:
            cpu: 500m
            memory: 2Gi

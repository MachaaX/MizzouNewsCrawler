apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: news-pipeline-template
  namespace: production
spec:
  serviceAccountName: argo-workflow
  entrypoint: pipeline

  # Define reusable templates for the pipeline
  templates:
  # Main pipeline orchestration (DAG) — allows overlap between discovery,
  # verification and extraction via polling-based gates.
  - name: pipeline
    inputs:
      parameters:
      - name: dataset
      - name: source-limit
        value: "10000"  # Default: effectively unlimited (process all sources)
      - name: max-articles
      - name: days-back
      - name: verify-batch-size
      - name: verify-max-batches
      - name: verify-idle-grace-seconds
        value: "600"
      - name: extract-limit
      - name: extract-batches
      - name: inter-request-min
      - name: inter-request-max
      - name: batch-sleep
      - name: captcha-backoff-base
      - name: captcha-backoff-max
        value: "7200"
      - name: min-candidates
        value: "50"
      - name: candidate-wait-seconds
        value: "300"  # 5 minutes
      - name: min-verified
        value: "25"
      - name: verified-wait-seconds
        value: "300"
      - name: log-level
        value: "INFO"
    dag:
      tasks:
      - name: discover-urls
        template: discovery-step
        arguments:
          parameters:
          - name: dataset
            value: "{{inputs.parameters.dataset}}"
          - name: source-limit
            value: "{{inputs.parameters.source-limit}}"
          - name: max-articles
            value: "{{inputs.parameters.max-articles}}"
          - name: days-back
            value: "{{inputs.parameters.days-back}}"
          - name: log-level
            value: "{{inputs.parameters.log-level}}"

      # Start a parallel wait task that will poll the DB for new candidate
      # links and only succeed when either min-candidates are present or
      # the candidate-wait-seconds timeout is reached.
      - name: wait-for-candidates
        template: wait-for-candidates
        arguments:
          parameters:
          - name: min-candidates
            value: "{{inputs.parameters.min-candidates}}"
          - name: timeout-seconds
            value: "{{inputs.parameters.candidate-wait-seconds}}"

      - name: verify-urls
        template: verification-step
        depends: "wait-for-candidates"
        arguments:
          parameters:
          - name: batch-size
            value: "{{inputs.parameters.verify-batch-size}}"
          - name: max-batches
            value: "{{inputs.parameters.verify-max-batches}}"
          - name: idle-grace-seconds
            value: "{{inputs.parameters.verify-idle-grace-seconds}}"
          - name: inter-request-min
            value: "{{inputs.parameters.inter-request-min}}"
          - name: inter-request-max
            value: "{{inputs.parameters.inter-request-max}}"
          - name: log-level
            value: "{{inputs.parameters.log-level}}"

      # Calculate optimal extraction parallelism first
      - name: calculate-extraction-workers
        template: calculate-extraction-workers

      - name: extract-content
        template: extraction-step
        depends: "calculate-extraction-workers"
        # No dependency - starts immediately in parallel with verification
        # Extraction will check for eligible articles and wait if none available
        arguments:
          parameters:
          - name: dataset
            value: "{{inputs.parameters.dataset}}"
          - name: limit
            value: "{{inputs.parameters.extract-limit}}"
          - name: batches
            value: "{{inputs.parameters.extract-batches}}"
          - name: inter-request-min
            value: "{{inputs.parameters.inter-request-min}}"
          - name: inter-request-max
            value: "{{inputs.parameters.inter-request-max}}"
          - name: batch-sleep
            value: "{{inputs.parameters.batch-sleep}}"
          - name: captcha-backoff-base
            value: "{{inputs.parameters.captcha-backoff-base}}"
          - name: captcha-backoff-max
            value: "{{inputs.parameters.captcha-backoff-max}}"
          - name: log-level
            value: "{{inputs.parameters.log-level}}"
        # Dynamic parallelism: spawn N workers based on calculation
        withParam: "{{tasks.calculate-extraction-workers.outputs.result}}"
  
  # Discovery step template
  - name: discovery-step
    inputs:
      parameters:
      - name: dataset
      - name: source-limit
        value: "10000"  # Default: effectively unlimited
      - name: max-articles
      - name: days-back
      - name: log-level
    metadata:
      labels:
        stage: discovery
        workflow-name: "{{workflow.name}}"
        workflow-uid: "{{workflow.uid}}"
    retryStrategy:
      limit: 2
      retryPolicy: "OnFailure"
      backoff:
        duration: "5m"
        factor: 2
        maxDuration: "20m"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/crawler:latest
      imagePullPolicy: IfNotPresent
      command:
        - python
        - -m
        - src.cli.cli_modular
        - discover-urls
        - --dataset
        - "{{inputs.parameters.dataset}}"
        - --source-limit
        - "{{inputs.parameters.source-limit}}"
        - --max-articles
        - "{{inputs.parameters.max-articles}}"
        - --days-back
        - "{{inputs.parameters.days-back}}"
        - --due-only
        - --log-level
        - "{{inputs.parameters.log-level}}"
      envFrom:
      - secretRef:
          name: origin-proxy-credentials
      env:
      # Database configuration
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Explicit TELEMETRY_DATABASE_URL to ensure telemetry uses PostgreSQL (not SQLite fallback)
      - name: TELEMETRY_DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Cloud SQL Connector
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      # Proxy configuration
      - name: PROXY_PROVIDER
        value: "decodo"
      - name: USE_ORIGIN_PROXY
        value: "false"
      - name: DECODO_SECRET_NAME
        value: "decodo-proxy-creds"
      - name: GOOGLE_CLOUD_PROJECT
        value: "mizzou-news-crawler"
      - name: NO_PROXY
        value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
      resources:
        requests:
          cpu: 200m
          memory: 2Gi
        limits:
          cpu: 1000m
          memory: 4Gi
  
  # Verification step template
  - name: verification-step
    inputs:
      parameters:
      - name: batch-size
      - name: max-batches
      - name: idle-grace-seconds
      - name: inter-request-min
      - name: inter-request-max
      - name: log-level
    metadata:
      labels:
        stage: verification
        workflow-name: "{{workflow.name}}"
        workflow-uid: "{{workflow.uid}}"
    retryStrategy:
      limit: 2
      retryPolicy: "OnFailure"
      backoff:
        duration: "5m"
        factor: 2
        maxDuration: "20m"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/crawler:latest
      imagePullPolicy: IfNotPresent
      command:
        - python
        - -m
        - src.cli.cli_modular
        - verify-urls
        - --batch-size
        - "{{inputs.parameters.batch-size}}"
        - --idle-grace-seconds
        - "{{inputs.parameters.idle-grace-seconds}}"
        - --log-level
        - "{{inputs.parameters.log-level}}"
        # No --max-batches: process entire backlog every run
        # Exit when no more URLs with status='discovered' remain
      envFrom:
      - secretRef:
          name: origin-proxy-credentials
      env:
      # Database configuration
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Explicit TELEMETRY_DATABASE_URL to ensure telemetry uses PostgreSQL (not SQLite fallback)
      - name: TELEMETRY_DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Cloud SQL Connector
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      # Proxy configuration
      - name: PROXY_PROVIDER
        value: "decodo"
      - name: USE_ORIGIN_PROXY
        value: "false"
      - name: DECODO_SECRET_NAME
        value: "decodo-proxy-creds"
      - name: GOOGLE_CLOUD_PROJECT
        value: "mizzou-news-crawler"
      - name: NO_PROXY
        value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
      # Rate limiting for verification
      - name: INTER_REQUEST_MIN
        value: "{{inputs.parameters.inter-request-min}}"
      - name: INTER_REQUEST_MAX
        value: "{{inputs.parameters.inter-request-max}}"
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
        limits:
          cpu: 1000m
          memory: 3Gi
  
  # Extraction step template
  - name: extraction-step
    inputs:
      parameters:
      - name: dataset
      - name: limit
      - name: batches
      - name: inter-request-min
      - name: inter-request-max
      - name: batch-sleep
      - name: captcha-backoff-base
      - name: captcha-backoff-max
      - name: log-level
    metadata:
      labels:
        stage: extraction
        workflow-name: "{{workflow.name}}"
        workflow-uid: "{{workflow.uid}}"
    retryStrategy:
      limit: 2
      retryPolicy: "OnFailure"
      backoff:
        duration: "5m"
        factor: 2
        maxDuration: "20m"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/crawler:latest
      imagePullPolicy: IfNotPresent
      command: ["/bin/bash", "-c"]
      args:
        - |
          echo "✅ Starting extraction immediately without waiting"
          echo "Note: Extract command will check for available articles internally"
          python -m src.cli.cli_modular extract \
            --dataset "{{inputs.parameters.dataset}}" \
            --limit "{{inputs.parameters.limit}}" \
            --batches "{{inputs.parameters.batches}}" \
            --log-level "{{inputs.parameters.log-level}}"
      envFrom:
      - secretRef:
          name: origin-proxy-credentials
      env:
      # Database configuration
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Explicit TELEMETRY_DATABASE_URL to ensure telemetry uses PostgreSQL (not SQLite fallback)
      - name: TELEMETRY_DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Cloud SQL Connector
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      # Work queue coordination
      - name: USE_WORK_QUEUE
        value: "true"
      - name: WORK_QUEUE_URL
        value: "http://work-queue.production.svc.cluster.local:8080"
      # Proxy configuration
      - name: PROXY_PROVIDER
        value: "decodo"
      - name: USE_ORIGIN_PROXY
        value: "false"
      - name: DECODO_SECRET_NAME
        value: "decodo-proxy-creds"
      - name: GOOGLE_CLOUD_PROJECT
        value: "mizzou-news-crawler"
      - name: SELENIUM_PROXY
        valueFrom:
          secretKeyRef:
            name: origin-proxy-credentials
            key: selenium-proxy-url
      # Decodo Unblock Proxy for strong bot protection (PerimeterX, DataDome)
      - name: UNBLOCK_PROXY_URL
        value: "https://unblock.decodo.com:60000"
      - name: UNBLOCK_PROXY_USER
        valueFrom:
          secretKeyRef:
            name: decodo-unblock-credentials
            key: username
      - name: UNBLOCK_PROXY_PASS
        valueFrom:
          secretKeyRef:
            name: decodo-unblock-credentials
            key: password
      # Rate limiting
      - name: INTER_REQUEST_MIN
        value: "{{inputs.parameters.inter-request-min}}"
      - name: INTER_REQUEST_MAX
        value: "{{inputs.parameters.inter-request-max}}"
      - name: BATCH_SLEEP_SECONDS
        value: "{{inputs.parameters.batch-sleep}}"
      - name: CAPTCHA_BACKOFF_BASE
        value: "{{inputs.parameters.captcha-backoff-base}}"
      - name: CAPTCHA_BACKOFF_MAX
        value: "{{inputs.parameters.captcha-backoff-max}}"
      # User agent rotation
      - name: UA_ROTATE_BASE
        value: "4"
      - name: UA_ROTATE_JITTER
        value: "0.25"
      # Decodo IP rotation
      - name: DECODO_ROTATE_IP
        value: "true"
      # Bypass proxy for internal services
      - name: NO_PROXY
        value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
        limits:
          cpu: 1000m
          memory: 3Gi

  # Wait template: poll DB until a minimum number of candidate links exist
  - name: wait-for-candidates
    inputs:
      parameters:
      - name: min-candidates
      - name: timeout-seconds
    metadata:
      labels:
        stage: wait-candidates
        workflow-name: "{{workflow.name}}"
        workflow-uid: "{{workflow.uid}}"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      imagePullPolicy: IfNotPresent
      command: ["/bin/sh","-c"]
      args:
        - |
          python - <<'PY'
          import time
          import sys
          from datetime import datetime, timedelta
          from src import config
          from sqlalchemy import create_engine, text

          min_candidates = int('{{inputs.parameters.min-candidates}}')
          timeout = int('{{inputs.parameters.timeout-seconds}}')
          deadline = datetime.utcnow() + timedelta(seconds=timeout)

          if getattr(config, 'USE_CLOUD_SQL_CONNECTOR', False) and getattr(config, 'CLOUD_SQL_INSTANCE', None):
              from src.models.cloud_sql_connector import create_cloud_sql_engine
              engine = create_cloud_sql_engine(
                  instance_connection_name=config.CLOUD_SQL_INSTANCE,
                  user=config.DATABASE_USER,
                  password=config.DATABASE_PASSWORD,
                  database=config.DATABASE_NAME,
              )
          else:
              engine = create_engine(config.DATABASE_URL)

          while True:
              try:
                  with engine.connect() as conn:
                      r = conn.execute(text("SELECT COUNT(*) FROM candidate_links WHERE created_at >= now() - interval '1 hour'"))
                      count = r.scalar() or 0
                      print(f"candidates_in_last_hour={count}")
                      if count >= min_candidates:
                          print("threshold met")
                          sys.exit(0)
              except Exception as e:
                  print(f"db check error: {e}")

              if datetime.utcnow() >= deadline:
                  print("timeout reached; proceeding")
                  sys.exit(0)

              time.sleep(15)
          PY
      env:
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"

  # Wait template: poll DB until a minimum number of verified articles exist
  - name: wait-for-verified
    inputs:
      parameters:
      - name: min-verified
      - name: timeout-seconds
    metadata:
      labels:
        stage: wait-verified
        workflow-name: "{{workflow.name}}"
        workflow-uid: "{{workflow.uid}}"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      imagePullPolicy: IfNotPresent
      command: ["/bin/sh","-c"]
      args:
        - |
          python - <<'PY'
          import time
          import sys
          from datetime import datetime, timedelta
          from src import config
          from sqlalchemy import create_engine, text

          min_verified = int('{{inputs.parameters.min-verified}}')
          timeout = int('{{inputs.parameters.timeout-seconds}}')
          deadline = datetime.utcnow() + timedelta(seconds=timeout)

          if getattr(config, 'USE_CLOUD_SQL_CONNECTOR', False) and getattr(config, 'CLOUD_SQL_INSTANCE', None):
              from src.models.cloud_sql_connector import create_cloud_sql_engine
              engine = create_cloud_sql_engine(
                  instance_connection_name=config.CLOUD_SQL_INSTANCE,
                  user=config.DATABASE_USER,
                  password=config.DATABASE_PASSWORD,
                  database=config.DATABASE_NAME,
              )
          else:
              engine = create_engine(config.DATABASE_URL)

          while True:
              try:
                  with engine.connect() as conn:
                      # Count verified candidate_links that don't have a corresponding article yet
                      r = conn.execute(text("""
                          SELECT COUNT(*) 
                          FROM candidate_links cl
                          WHERE cl.status = 'article'
                          AND NOT EXISTS (
                              SELECT 1 
                              FROM articles a
                              WHERE a.candidate_link_id = cl.id
                          )
                      """))
                      count = r.scalar() or 0
                      print(f"articles_ready_for_extraction={count}")
                      if count >= min_verified:
                          print(f"threshold met: {count} >= {min_verified}")
                          sys.exit(0)
                      elif count > 0:
                          print(f"found {count} articles, waiting for {min_verified}")
              except Exception as e:
                  print(f"db check error: {e}")

              if datetime.utcnow() >= deadline:
                  print("timeout reached; proceeding")
                  sys.exit(0)

              time.sleep(15)
          PY
      env:
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"

  # Calculate optimal extraction workers based on backlog
  - name: calculate-extraction-workers
    metadata:
      labels:
        stage: calculate-workers
        workflow-name: "{{workflow.name}}"
        workflow-uid: "{{workflow.uid}}"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      imagePullPolicy: IfNotPresent
      command: ["/bin/sh", "-c"]
      args:
        - |
          python - <<'PY'
          import json
          import sys
          from src import config
          from sqlalchemy import text

          # Use Cloud SQL Connector (no sidecar needed)
          if getattr(config, 'USE_CLOUD_SQL_CONNECTOR', False) and getattr(config, 'CLOUD_SQL_INSTANCE', None):
              from src.models.cloud_sql_connector import create_cloud_sql_engine
              engine = create_cloud_sql_engine(
                  instance_connection_name=config.CLOUD_SQL_INSTANCE,
                  user=config.DATABASE_USER,
                  password=config.DATABASE_PASSWORD,
                  database=config.DATABASE_NAME,
              )
          else:
              from sqlalchemy import create_engine
              engine = create_engine(config.DATABASE_URL)

          def get_extraction_backlog():
              with engine.connect() as conn:
                  # Optimized: NOT EXISTS is 20-40x faster than NOT IN
                  result = conn.execute(text("""
                      SELECT COUNT(*)
                      FROM candidate_links cl
                      WHERE cl.status = 'article'
                      AND NOT EXISTS (
                          SELECT 1 FROM articles a
                          WHERE a.candidate_link_id = cl.id
                      )
                  """)).scalar()
                  return result or 0

          def calculate_worker_count(backlog):
              # Scale workers proportionally: 2 workers per 100 articles
              # n=100 → 2 workers, n=500 → 10 workers
              if backlog < 50:
                  return 1
              workers = min(10, max(2, (backlog // 100) * 2))
              return workers

          backlog = get_extraction_backlog()
          worker_count = calculate_worker_count(backlog)
          
          # Output JSON array [{"id": 1}, {"id": 2}, ...] for Argo withParam
          workers = [{"id": i+1} for i in range(worker_count)]
          print(f"Backlog: {backlog} articles → {worker_count} workers", file=sys.stderr)
          print(json.dumps(workers))
          PY
      env:
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@localhost/$(DATABASE_NAME)"
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      envFrom:
      - secretRef:
          name: origin-proxy-credentials

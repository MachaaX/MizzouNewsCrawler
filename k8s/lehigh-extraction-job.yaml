apiVersion: batch/v1
kind: Job
metadata:
  name: lehigh-extraction
  namespace: production
spec:
  ttlSecondsAfterFinished: 86400  # Clean up after 24 hours
  template:
    metadata:
      labels:
        app: lehigh-extraction
    spec:
      serviceAccountName: mizzou-app
      priorityClassName: batch-standard  # Can be preempted by services/cron jobs
      restartPolicy: Never
      containers:
      - name: extraction
        image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:f2738fb
        imagePullPolicy: Always
        command:
          - python
          - -m
          - src.cli.cli_modular
          - extract
          - --dataset
          - Penn-State-Lehigh
          - --limit
          - "3"  # 3 articles per batch
          - --batches
          - "1"  # Just one batch
        env:
        # Database configuration
        - name: DATABASE_ENGINE
          value: "postgresql+psycopg2"
        - name: DATABASE_HOST
          value: "127.0.0.1"
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: username
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: password
        - name: DATABASE_NAME
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: database
        # Cloud SQL Connector
        - name: USE_CLOUD_SQL_CONNECTOR
          value: "true"
        - name: CLOUD_SQL_INSTANCE
          value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
        # Proxy configuration
        - name: PROXY_PROVIDER
          value: "squid"
        - name: SQUID_PROXY_URL
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: squid-proxy-url
              optional: true
        - name: SQUID_PROXY_USERNAME
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: username
              optional: true
        - name: SQUID_PROXY_PASSWORD
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: password
              optional: true
        - name: SELENIUM_PROXY
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: selenium-proxy-url
              optional: true
        # EXTREME rate limiting for maximum bot protection avoidance
        # Lehigh Valley News has VERY aggressive bot detection - bot sensitivity level 10
        - name: INTER_REQUEST_MIN
          value: "90.0"  # increase minimum to 90s
        - name: INTER_REQUEST_MAX
          value: "180.0"  # increase maximum to 180s (3 minutes)
        - name: CAPTCHA_BACKOFF_BASE
          value: "7200"  # Was 2400 - increased to 2 hours
        - name: CAPTCHA_BACKOFF_MAX
          value: "21600"  # Was 7200 - increased to 6 hours
        - name: BATCH_SLEEP_SECONDS
          value: "420.0"  # 7 minutes between batches (auto-detected single-domain)
        - name: BATCH_SLEEP_JITTER
          value: "0.45"  # 45% jitter (~231-609 seconds range)
        - name: BATCH_SIZE_JITTER
          value: "0.33"  # 33% jitter (3 Â± 1 = 2-4 articles per batch)
        # User agent rotation (rotate every 3-5 requests)
        - name: UA_ROTATE_BASE
          value: "4"
        - name: UA_ROTATE_JITTER
          value: "0.25"
        # Bypass proxy for internal services
        - name: NO_PROXY
          value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
        resources:
          requests:
            cpu: 100m
            memory: 768Mi
          limits:
            cpu: 500m
            memory: 2Gi

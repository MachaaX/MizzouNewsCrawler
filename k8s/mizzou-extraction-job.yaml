apiVersion: batch/v1
kind: Job
metadata:
  name: mizzou-extraction
  namespace: production
  labels:
    app: mizzou-extraction
    dataset: mizzou-missouri-state
    type: extraction
spec:
  ttlSecondsAfterFinished: 86400  # Clean up after 24 hours
  template:
    metadata:
      labels:
        app: mizzou-extraction
        dataset: mizzou-missouri-state
        type: extraction
    spec:
      serviceAccountName: mizzou-app
      priorityClassName: batch-standard  # Can be preempted by services/cron jobs
      restartPolicy: Never
      containers:
      - name: extraction
        image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
        imagePullPolicy: Always
        command:
          - python
          - -m
          - src.cli.cli_modular
          - extract
          - --dataset
          - "Mizzou Missouri State"
          - --limit
          - "20"  # 20 articles per batch
          - --batches
          - "60"  # 60 batches = ~1200 articles max
        env:
        # Database configuration
        - name: DATABASE_ENGINE
          value: "postgresql+psycopg2"
        - name: DATABASE_HOST
          value: "127.0.0.1"
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: username
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: password
        - name: DATABASE_NAME
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: database
        # Cloud SQL Connector
        - name: USE_CLOUD_SQL_CONNECTOR
          value: "true"
        - name: CLOUD_SQL_INSTANCE
          value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
        # Proxy configuration
        - name: PROXY_PROVIDER
          value: "squid"
        - name: SQUID_PROXY_URL
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: squid-proxy-url
              optional: true
        - name: SQUID_PROXY_USERNAME
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: username
              optional: true
        - name: SQUID_PROXY_PASSWORD
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: password
              optional: true
        - name: SELENIUM_PROXY
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: selenium-proxy-url
              optional: true
        # Moderate rate limiting for Mizzou (faster than Lehigh)
        - name: INTER_REQUEST_MIN
          value: "5.0"   # 5 seconds min
        - name: INTER_REQUEST_MAX
          value: "15.0"  # 15 seconds max
        - name: BATCH_SLEEP_SECONDS
          value: "30.0"  # 30 seconds between batches
        - name: CAPTCHA_BACKOFF_BASE
          value: "1800"  # 30 minutes base backoff
        - name: CAPTCHA_BACKOFF_MAX
          value: "7200"  # 2 hours max backoff
        # User agent rotation
        - name: UA_ROTATE_BASE
          value: "4"
        - name: UA_ROTATE_JITTER
          value: "0.25"
        # Bypass proxy for internal services
        - name: NO_PROXY
          value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
        resources:
          requests:
            cpu: 250m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 3Gi

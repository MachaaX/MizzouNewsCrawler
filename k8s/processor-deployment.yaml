apiVersion: apps/v1
kind: Deployment
metadata:
  name: mizzou-processor
  namespace: production
  labels:
    app: mizzou-processor
    component: processor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mizzou-processor
  template:
    metadata:
      labels:
        app: mizzou-processor
        component: processor
    spec:
      serviceAccountName: mizzou-app
      priorityClassName: service-standard  # High priority, can preempt batch jobs
      containers:
      - name: processor
        image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:${PROCESSOR_TAG}
        imagePullPolicy: IfNotPresent
        command:
          - python
          - orchestration/continuous_processor.py
        env:
        # Database configuration
        - name: DATABASE_ENGINE
          value: "postgresql+psycopg2"
        - name: DATABASE_HOST
          value: "127.0.0.1"
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: username
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: password
        - name: DATABASE_NAME
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: database
        # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
        - name: DATABASE_URL
          value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
        # Explicit TELEMETRY_DATABASE_URL to ensure telemetry uses PostgreSQL (not SQLite fallback)
        - name: TELEMETRY_DATABASE_URL
          value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
        # Cloud SQL Connector configuration
        - name: USE_CLOUD_SQL_CONNECTOR
          value: "true"
        - name: CLOUD_SQL_INSTANCE
          value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
        # Processor configuration
        - name: POLL_INTERVAL
          value: "60"
        - name: VERIFICATION_BATCH_SIZE
          value: "10"
        - name: EXTRACTION_BATCH_SIZE
          value: "10"  # Reduced from 20 for better bot evasion
        - name: ANALYSIS_BATCH_SIZE
          value: "128"  # Increased from 16 to catch up with extraction rate
        - name: GAZETTEER_BATCH_SIZE
          value: "50"
        # Pipeline step feature flags (Issue #77 refactoring)
        # PHASE 2: Disable discovery/verification/extraction - moved to dataset jobs
        # Processor now handles ONLY internal processing (cleaning/ML/entities)
        - name: ENABLE_DISCOVERY
          value: "false"  # Phase 2: DISABLED - moved to dataset jobs
        - name: ENABLE_VERIFICATION
          value: "false"  # Phase 2: DISABLED - moved to dataset jobs
        - name: ENABLE_EXTRACTION
          value: "false"  # Phase 2: DISABLED - moved to dataset jobs
        - name: ENABLE_CLEANING
          value: "true"   # Keep in continuous processor
        - name: ENABLE_ML_ANALYSIS
          value: "true"   # Keep in continuous processor
        - name: ENABLE_ENTITY_EXTRACTION
          value: "true"   # Re-enabled after gazetteer caching fix (865c812)
        # CAPTCHA and rate limit handling - CONSERVATIVE SETTINGS (match Lehigh)
        - name: CAPTCHA_BACKOFF_BASE
          value: "7200"  # 2 hours (3x longer than before)
        - name: CAPTCHA_BACKOFF_MAX
          value: "21600"  # 6 hours (3x longer than before)
        - name: INTER_REQUEST_MIN
          value: "90.0"  # 90 seconds (6x slower for bot evasion)
        - name: INTER_REQUEST_MAX
          value: "180.0"  # 3 minutes (7x slower for bot evasion)
        - name: BATCH_SLEEP_SECONDS
          value: "420.0"  # 7 minutes between batches (match Lehigh)
        - name: BATCH_SLEEP_JITTER
          value: "0.45"  # 45% jitter (~231-609 seconds range)
        - name: BATCH_SIZE_JITTER
          value: "0.20"  # 20% jitter on batch size (8-12 articles)
        # User agent rotation (rotate every 3-5 requests)
        - name: UA_ROTATE_BASE
          value: "4"
        - name: UA_ROTATE_JITTER
          value: "0.25"
        # Proxy configuration - Squid-only
        - name: PROXY_PROVIDER
          value: "squid"
        - name: SQUID_PROXY_URL
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: squid-proxy-url
              optional: true
        - name: SQUID_PROXY_USERNAME
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: username
              optional: true
        - name: SQUID_PROXY_PASSWORD
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: password
              optional: true
        - name: SELENIUM_PROXY
          valueFrom:
            secretKeyRef:
              name: squid-proxy-credentials
              key: selenium-proxy-url
              optional: true
        - name: MEDIACLOUD_SECRET_NAME
          value: "mediacloud-api-token"
        - name: GOOGLE_CLOUD_PROJECT
          value: "mizzou-news-crawler"
        # Bypass proxy for HuggingFace model downloads and internal services
        - name: NO_PROXY
          value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co,cdn-lfs.huggingface.co,cdn.huggingface.co"
        - name: no_proxy
          value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co,cdn-lfs.huggingface.co,cdn.huggingface.co"
        resources:
          requests:
            cpu: 100m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 4Gi
        livenessProbe:
          exec:
            command:
              - /bin/sh
              - -c
              - "pgrep -f 'continuous_processor.py' > /dev/null"
          initialDelaySeconds: 30
          periodSeconds: 60
          timeoutSeconds: 5
          failureThreshold: 5
        readinessProbe:
          exec:
            command:
              - /bin/sh
              - -c
              - "pgrep -f 'continuous_processor.py' > /dev/null"
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 3
          failureThreshold: 3
